################## Vocabulary ##################
# Default word tokens for vocabulary
PAD_token: 0  # Used for padding short sentences
SOS_token: 1  # Start-of-sentence token
EOS_token: 2  # End of sentence token

# Maximum sentence length to consider in the vocabulary
max_length: 10

# Minimum word count threshold for trimming in the vocabulary
min_count:  3

################## Model ##################
# Name of chatbot model
model_name: 'cb_model'

# Name of the corpus used
corpus_name: "cornell movie-dialogs corpus"

# Which score to use in the attention for the decoder
attn_model: 'dot' # general or concat

# Size of the hidden layer for both encoder and decoder
hidden_size: 500

# Number of layers in Encoder
encoder_n_layers:  2

# Number of layers in Decoder
decoder_n_layers:  2

# Dropout percentage in decoder
dropout:  0.1

# Sampling
sampling: True # TURN FALSE WHILE TRAINING

# Sampling temperature used in decoder
temperature: 0.7

################## Training ##################
# Batch size during training
batch_size: 64

# Set checkpoint to load from; set to None if starting from scratch
checkpoint_iter: 4000

# Clip gradients
clip: 50.0

# What is the probability for doing teacher forcing
teacher_forcing_ratio: 1.0

# Learning rate encoder
learning_rate: 0.0001

# How many times bigger learning rate for decoder than encoder
decoder_learning_ratio: 5.0

# Number of epochs of training
n_iteration: 4001

# Print average loss every x epoch
print_every: 100

# Save model every x epoch
save_every: 500


